{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469e11fc-e346-4335-8342-8bcc78915c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import datetime\n",
    "from torch.utils.data import ConcatDataset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c50b0a-1cb0-44fc-b652-831299d5eff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b49acab-517d-40f6-a189-287c9f877019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 1: Define Dataset Wrapper ----\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ---- STEP 2: Define the Incremental Model ----\n",
    "class IncrementalResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(IncrementalResNet, self).__init__()\n",
    "        base_model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # remove fc\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # with torch.no_grad():\n",
    "        #     features = self.feature_extractor(x)\n",
    "        #     features = torch.flatten(features, 1)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = torch.flatten(features, 1)\n",
    "        logits = self.fc(features)\n",
    "        return logits\n",
    "\n",
    "    def add_classes(self, num_new):\n",
    "        old_weights = self.fc.weight.data.clone()\n",
    "        old_bias = self.fc.bias.data.clone()\n",
    "\n",
    "        new_fc = nn.Linear(2048, self.fc.out_features + num_new)\n",
    "        new_fc.weight.data[:self.fc.out_features] = old_weights\n",
    "        new_fc.bias.data[:self.fc.out_features] = old_bias\n",
    "\n",
    "        self.fc = new_fc\n",
    "\n",
    "# ---- STEP 3: Training Function ----\n",
    "def run_epoch(phase, model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for imgs, labels in dataloader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    return { \n",
    "        'loss': running_loss / num_samples, \n",
    "        'acc': running_corrects.double() / num_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57edad94-bcf2-4ec2-a8ad-a73e1403cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../insect-dataset/lepidoptera\"\n",
    "\n",
    "image_size = 224\n",
    "img_header_footer_ratio = 1.1\n",
    "normazile_x = [0.485, 0.456, 0.406]\n",
    "normalize_y = [0.229, 0.224, 0.225]\n",
    "\n",
    "def_transform = [\n",
    "    transforms.Resize(int(image_size * img_header_footer_ratio)),\n",
    "    transforms.CenterCrop((image_size, image_size)),\n",
    "    transforms.RandomRotation(15, fill=(0, 0, 0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(normazile_x, normalize_y),\n",
    "]\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50401865-8609-48d7-be31-dc7ef6e40fa8",
   "metadata": {},
   "source": [
    "# Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2333da-c98a-4c9a-b5d3-8c9633350944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_i1_classes = 21\n",
      "num_i1_train_images = 2460\n",
      "num_i1_val_images = 46\n"
     ]
    }
   ],
   "source": [
    "i1_train_dir = f\"{dataset_dir}/i00001-train\"\n",
    "i1_val_dir = f\"{dataset_dir}/i00001-val\"\n",
    "\n",
    "i1_classes = os.listdir(i1_train_dir)\n",
    "num_i1_classes = len(i1_classes)\n",
    "print(f\"num_i1_classes = {num_i1_classes}\")\n",
    "\n",
    "i1_train_images = [ \n",
    "    (f\"{i1_train_dir}/{class_dir}/{img}\", i1_classes.index(class_dir)) \n",
    "    for class_dir in os.listdir(i1_train_dir) \n",
    "    for img in os.listdir(f\"{i1_train_dir}/{class_dir}\")\n",
    "]\n",
    "num_i1_train_images = len(i1_train_images)\n",
    "print(f\"num_i1_train_images = {num_i1_train_images}\")\n",
    "i1_train_dataset = SimpleImageDataset(\n",
    "    image_paths = [ t[0] for t in i1_train_images],\n",
    "    labels = [ t[1] for t in i1_train_images],\n",
    "    transform = transforms.Compose(def_transform)\n",
    ")\n",
    "i1_train_loader = DataLoader(i1_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i1_val_images = [ \n",
    "    (f\"{i1_val_dir}/{class_dir}/{img}\", i1_classes.index(class_dir)) \n",
    "    for class_dir in os.listdir(i1_val_dir) \n",
    "    for img in os.listdir(f\"{i1_val_dir}/{class_dir}\")\n",
    "]\n",
    "num_i1_val_images = len(i1_val_images)\n",
    "print(f\"num_i1_val_images = {num_i1_val_images}\")\n",
    "i1_val_dataset = SimpleImageDataset(\n",
    "    image_paths = [ t[0] for t in i1_val_images],\n",
    "    labels = [ t[1] for t in i1_val_images],\n",
    "    transform = transforms.Compose(def_transform)\n",
    ")\n",
    "i1_val_loader = DataLoader(i1_val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5780068-2287-4e2d-97b1-ec12c15832de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i1.e001 | TRAIN loss:1.355 acc:0.632 | VAL loss: 1.047 acc:0.696 | Elapsed time: 0:00:34.466492\n",
      "i1.e002 | TRAIN loss:0.342 acc:0.885 | VAL loss: 0.537 acc:0.804 | Elapsed time: 0:01:10.626994\n",
      "i1.e003 | TRAIN loss:0.174 acc:0.947 | VAL loss: 0.318 acc:0.870 | Elapsed time: 0:01:49.678433\n",
      "i1.e004 | TRAIN loss:0.106 acc:0.963 | VAL loss: 0.309 acc:0.935 | Elapsed time: 0:02:30.762869\n",
      "i1.e005 | TRAIN loss:0.060 acc:0.985 | VAL loss: 0.188 acc:0.957 | Elapsed time: 0:03:12.476010\n",
      "i1.e006 | TRAIN loss:0.068 acc:0.978 | VAL loss: 0.168 acc:0.957 | Elapsed time: 0:03:55.005025\n",
      "i1.e007 | TRAIN loss:0.044 acc:0.987 | VAL loss: 0.046 acc:1.000 | Elapsed time: 0:04:38.296883\n",
      "i1.e008 | TRAIN loss:0.038 acc:0.989 | VAL loss: 0.046 acc:0.978 | Elapsed time: 0:05:22.042816\n",
      "i1.e009 | TRAIN loss:0.027 acc:0.991 | VAL loss: 0.039 acc:1.000 | Elapsed time: 0:06:06.707914\n",
      "i1.e010 | TRAIN loss:0.030 acc:0.991 | VAL loss: 0.044 acc:1.000 | Elapsed time: 0:06:52.235945\n"
     ]
    }
   ],
   "source": [
    "model = IncrementalResNet(num_classes=num_i1_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(10):\n",
    "    print(f\"i1.e{epoch+1:03} | \", end='')\n",
    "    \n",
    "    train_result = run_epoch('train', model, i1_train_loader, optimizer, criterion, device)\n",
    "    print(f\"TRAIN loss:{train_result['loss']:.3f} acc:{train_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    val_result = run_epoch('val', model, i1_val_loader, optimizer, criterion, device)\n",
    "    print(f\"VAL loss: {val_result['loss']:.3f} acc:{val_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    print(f\"Elapsed time: {datetime.timedelta(seconds=(time.time() - start_time))}\")\n",
    "\n",
    "i1_checkpoint = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b4d69b-9d75-4874-bfee-e953a1e1e9d5",
   "metadata": {},
   "source": [
    "# Iteration 2\n",
    "no overlapping classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38d003e-6be0-4a6b-9f98-5807996653ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_i2_classes = 19\n",
      "num_i2_train_images = 916\n",
      "num_i2_val_images = 42\n"
     ]
    }
   ],
   "source": [
    "i2_train_dir = f\"{dataset_dir}/i00002-train\"\n",
    "i2_val_dir = f\"{dataset_dir}/i00002-val\"\n",
    "\n",
    "i2_classes = os.listdir(i2_train_dir)\n",
    "num_i2_classes = len(i2_classes)\n",
    "print(f\"num_i2_classes = {num_i2_classes}\")\n",
    "\n",
    "i2_train_images = [ \n",
    "    (f\"{i2_train_dir}/{class_dir}/{img}\", num_i1_classes + i2_classes.index(class_dir)) \n",
    "    for class_dir in os.listdir(i2_train_dir) \n",
    "    for img in os.listdir(f\"{i2_train_dir}/{class_dir}\")\n",
    "]\n",
    "num_i2_train_images = len(i2_train_images)\n",
    "print(f\"num_i2_train_images = {num_i2_train_images}\")\n",
    "i2_train_dataset = SimpleImageDataset(\n",
    "    image_paths = [ t[0] for t in i2_train_images],\n",
    "    labels = [ t[1] for t in i2_train_images],\n",
    "    transform = transforms.Compose(def_transform)\n",
    ")\n",
    "i2_train_loader = DataLoader(i2_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i2_val_images = [ \n",
    "    (f\"{i2_val_dir}/{class_dir}/{img}\", num_i1_classes + i2_classes.index(class_dir)) \n",
    "    for class_dir in os.listdir(i2_val_dir) \n",
    "    for img in os.listdir(f\"{i2_val_dir}/{class_dir}\")\n",
    "]\n",
    "num_i2_val_images = len(i2_val_images)\n",
    "print(f\"num_i2_val_images = {num_i2_val_images}\")\n",
    "i2_val_dataset = SimpleImageDataset(\n",
    "    image_paths = [ t[0] for t in i2_val_images],\n",
    "    labels = [ t[1] for t in i2_val_images],\n",
    "    transform = transforms.Compose(def_transform)\n",
    ")\n",
    "i2_val_loader = DataLoader(i2_val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5fc24-0e18-4e13-9d16-33b8daa19440",
   "metadata": {},
   "source": [
    "## with only i1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106a4052-5ada-417e-b977-c86ed784187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new classes. New classifier size: 40\n",
      "i2.e001 | TRAIN loss:1.833 acc:0.517 | i1-VAL loss:1.812 acc:0.500 | i2-VAL loss:1.867 acc:0.500 | Elapsed time: 0:00:18.000814\n",
      "i2.e002 | TRAIN loss:0.452 acc:0.892 | i1-VAL loss:2.492 acc:0.326 | i2-VAL loss:0.833 acc:0.833 | Elapsed time: 0:00:35.832851\n",
      "i2.e003 | TRAIN loss:0.129 acc:0.971 | i1-VAL loss:2.839 acc:0.261 | i2-VAL loss:0.427 acc:0.905 | Elapsed time: 0:00:54.128214\n",
      "i2.e004 | TRAIN loss:0.061 acc:0.990 | i1-VAL loss:3.356 acc:0.283 | i2-VAL loss:0.237 acc:0.952 | Elapsed time: 0:01:12.253101\n",
      "i2.e005 | TRAIN loss:0.031 acc:0.995 | i1-VAL loss:3.157 acc:0.239 | i2-VAL loss:0.313 acc:0.905 | Elapsed time: 0:01:30.693455\n"
     ]
    }
   ],
   "source": [
    "model = copy.deepcopy(i1_checkpoint)\n",
    "model.add_classes(num_new=num_i2_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(\"Added new classes. New classifier size:\", model.fc.out_features)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(5):\n",
    "    print(f\"i2.e{epoch+1:03} | \", end='')\n",
    "    \n",
    "    train_result = run_epoch('train', model, i2_train_loader, optimizer, criterion, device)\n",
    "    print(f\"TRAIN loss:{train_result['loss']:.3f} acc:{train_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    val_result = run_epoch('val', model, i1_val_loader, optimizer, criterion, device)\n",
    "    print(f\"i1-VAL loss:{val_result['loss']:.3f} acc:{val_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    val_result = run_epoch('val', model, i2_val_loader, optimizer, criterion, device)\n",
    "    print(f\"i2-VAL loss:{val_result['loss']:.3f} acc:{val_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    print(f\"Elapsed time: {datetime.timedelta(seconds=(time.time() - start_time))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f647335-b2f9-47cd-bc67-f1b870bfe1b1",
   "metadata": {},
   "source": [
    "CATASTROPHIC FORGETTING... i1-VAL.acc down to 23.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018a326-5e2c-4efe-87b1-f8aad6e59a61",
   "metadata": {},
   "source": [
    "## with i1+i2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cca151c-028e-48a8-abc6-78bb322ae99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new classes. New classifier size: 40\n",
      "i2.e001 | TRAIN loss:0.351 acc:0.909 | i1-VAL loss:0.206 acc:0.913 | i2-VAL loss:1.163 acc:0.714 | Elapsed time: 0:01:03.489736\n",
      "i2.e002 | TRAIN loss:0.090 acc:0.973 | i1-VAL loss:0.257 acc:0.935 | i2-VAL loss:0.662 acc:0.857 | Elapsed time: 0:02:07.336270\n",
      "i2.e003 | TRAIN loss:0.058 acc:0.987 | i1-VAL loss:0.012 acc:1.000 | i2-VAL loss:0.308 acc:0.952 | Elapsed time: 0:03:11.821860\n",
      "i2.e004 | TRAIN loss:0.033 acc:0.992 | i1-VAL loss:0.071 acc:0.978 | i2-VAL loss:0.278 acc:0.929 | Elapsed time: 0:04:15.221807\n",
      "i2.e005 | TRAIN loss:0.043 acc:0.988 | i1-VAL loss:0.056 acc:0.978 | i2-VAL loss:0.189 acc:1.000 | Elapsed time: 0:05:18.450692\n"
     ]
    }
   ],
   "source": [
    "model = copy.deepcopy(i1_checkpoint)\n",
    "model.add_classes(num_new=num_i2_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(\"Added new classes. New classifier size:\", model.fc.out_features)\n",
    "\n",
    "combined_train_dataset = ConcatDataset([i1_train_dataset, i2_train_dataset])\n",
    "combined_train_loader = DataLoader(combined_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(5):\n",
    "    print(f\"i2.e{epoch+1:03} | \", end='')\n",
    "    \n",
    "    train_result = run_epoch('train', model, combined_train_loader, optimizer, criterion, device)\n",
    "    print(f\"TRAIN loss:{train_result['loss']:.3f} acc:{train_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    val_result = run_epoch('val', model, i1_val_loader, optimizer, criterion, device)\n",
    "    print(f\"i1-VAL loss:{val_result['loss']:.3f} acc:{val_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    val_result = run_epoch('val', model, i2_val_loader, optimizer, criterion, device)\n",
    "    print(f\"i2-VAL loss:{val_result['loss']:.3f} acc:{val_result['acc']:.3f} | \", end='')\n",
    "    \n",
    "    print(f\"Elapsed time: {datetime.timedelta(seconds=(time.time() - start_time))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d3e3c-20e3-4595-8978-c62872166d54",
   "metadata": {},
   "source": [
    "## with only i1 data + using distillation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e91f793-6502-4468-956d-f6e99f1985ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    student_log_probs = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "\n",
    "def run_epoch_v2(phase, model, dataloader, optimizer, criterion, device, teacher_model=None, distill_lambda=1.0, temperature=2.0):\n",
    "    model.train() if phase == 'train' else model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in dataloader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Add distillation loss if teacher is given\n",
    "            if teacher_model is not None and phase == 'train':\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(imgs)\n",
    "                loss += distill_lambda * distillation_loss(outputs, teacher_outputs, temperature)\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_correct += torch.sum(preds == labels).item()\n",
    "        total_samples += imgs.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / total_samples,\n",
    "        \"acc\": total_correct / total_samples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf21e07-e83d-4cbe-85d9-d606d55af365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new classes. New classifier size: 40\n",
      "i2.e001 | TRAIN loss:2.862 acc:0.456 | i1-VAL loss:0.290 acc:1.000 | i2-VAL loss:1.826 acc:0.619 | Elapsed time: 0:37:28.882383\n",
      "i2.e002 | TRAIN loss:1.825 acc:0.824 | i1-VAL loss:0.236 acc:1.000 | i2-VAL loss:0.929 acc:0.833 | Elapsed time: 0:37:47.412774\n",
      "i2.e003 | TRAIN loss:1.524 acc:0.927 | i1-VAL loss:0.224 acc:1.000 | i2-VAL loss:0.606 acc:0.952 | Elapsed time: 0:38:06.830109\n",
      "i2.e004 | TRAIN loss:1.423 acc:0.950 | i1-VAL loss:0.260 acc:0.978 | i2-VAL loss:0.511 acc:0.952 | Elapsed time: 0:38:26.658125\n",
      "i2.e005 | TRAIN loss:1.368 acc:0.955 | i1-VAL loss:0.299 acc:1.000 | i2-VAL loss:0.464 acc:1.000 | Elapsed time: 0:38:46.674804\n"
     ]
    }
   ],
   "source": [
    "model = copy.deepcopy(i1_checkpoint)\n",
    "model.add_classes(num_new=num_i2_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(\"Added new classes. New classifier size:\", model.fc.out_features)\n",
    "\n",
    "teacher_model = copy.deepcopy(model)\n",
    "teacher_model.eval()\n",
    "for p in teacher_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"i2.e{epoch+1:03} | \", end='')\n",
    "\n",
    "    train_result = run_epoch_v2('train', model, i2_train_loader, optimizer, criterion, device, \n",
    "                                teacher_model=teacher_model, distill_lambda=1.0, temperature=2.0)\n",
    "    print(f\"TRAIN loss:{train_result['loss']:.3f} acc:{train_result['acc']:.3f} | \", end='')\n",
    "\n",
    "    val_result_i1 = run_epoch_v2('val', model, i1_val_loader, optimizer, criterion, device, \n",
    "                                teacher_model=None, distill_lambda=1.0, temperature=2.0)\n",
    "    print(f\"i1-VAL loss:{val_result_i1['loss']:.3f} acc:{val_result_i1['acc']:.3f} | \", end='')\n",
    "\n",
    "    val_result_i2 = run_epoch_v2('val', model, i2_val_loader, optimizer, criterion, device, \n",
    "                                teacher_model=None, distill_lambda=1.0, temperature=2.0)\n",
    "    print(f\"i2-VAL loss:{val_result_i2['loss']:.3f} acc:{val_result_i2['acc']:.3f} | \", end='')\n",
    "    \n",
    "    print(f\"Elapsed time: {datetime.timedelta(seconds=(time.time() - start_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720aa6f2-d105-480b-ae11-e57bf326e0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
